# 🚀 PX4 Autopilot Task Planning with LLM & MAVSDK  
**Author:** Ibrahim Odat  
**Date:** 03.24.2025  

---

## 📘 Project Overview

This research explores the integration of **PX4 Autopilot**, **MAVSDK**, and **Large Language Models (LLMs)** to enable autonomous drone mission planning using high-level natural language commands. The system is inspired by the TypeFly architecture but is extended for **PX4 SITL** with object detection using **YOLOv8** and **scene understanding using VLMs** (e.g., LLaVA or LLAMA3.2-Vision).

---

## 🧠 System Architecture


---

## 🛠️ MAVSDK Task Templates & Capabilities

Before integrating LLMs, all core PX4 functionalities must be **validated manually** via MAVSDK in SITL. Below are the templates and user-facing tasks we identified:

### ✅ Core Task Templates

| Task Category         | Example Missions                                    |
|-----------------------|-----------------------------------------------------|
| **Takeoff / Hover**   | Take off to 10m, Hover for 5s                       |
| **Go To / Waypoints** | Fly forward 10m, Move to object, Navigate GPS path |
| **Land / RTL**        | Land in place, Return to Launch                    |
| **Survey / Grid**     | Fly 3x3 grid at 20m, Inspect point A, B, C         |
| **Object Detection**  | Go to red car, Circle object, Track moving person  |
| **Failsafe / Emergency** | Trigger RTL on low battery, Emergency land       |

These tasks form the **foundation** for all future LLM-generated plans.

---

## 🔍 Key Findings

- ✅ **MAVSDK covers nearly all drone mission skills needed** for PX4 integration.
- ✅ Tasks like `arm()`, `takeoff()`, `goto_location()`, `return_to_launch()`, and `land()` are fully supported via Python API.
- ✅ YOLOv8 provides object position (x, y, depth), which can be **converted to relative/NED coordinates or GPS** for use with MAVSDK.
- ✅ VLMs (like LLaVA) help convert object detection results into rich scene descriptions for the LLM.
- ✅ LLMs (like LLAMA or GPT) can generate valid MAVSDK mission code **if prompt-engineered with system capabilities**.

---

## 🧪 Next Step: Task Validation Phase

Before LLM integration, we will **validate each MAVSDK skill** independently using PX4 SITL. This includes:

- [x] Takeoff & Hover  
- [x] Navigate to GPS / Relative Positions  
- [x] Waypoint Missions  
- [x] Land / RTL  
- [x] Emergency RTL / Failsafe Tests  
- [x] Object-Centric Navigation via Relative Position  

Each test will include:
- Connection to SITL
- Execution of command
- Logging of result
- Pass/fail status

Once validated, we will proceed to build the **LLM Code Planner** and close the loop with full automation.

---

## ✅ Conclusion

This project demonstrates the feasibility of building an LLM-powered PX4 mission planner by leveraging **YOLO for vision**, **VLM for scene understanding**, and **MAVSDK for control**. The next step is critical — validating the foundation. Once done, we’ll unlock autonomous, vision-guided drone operations using natural language.

> 🚁 "Tell the drone: Fly to the red car and hover.  
> The system makes it real."

---

**Repo maintained by:** Ibrahim Odat  
**Date:** 03.23.2025  

